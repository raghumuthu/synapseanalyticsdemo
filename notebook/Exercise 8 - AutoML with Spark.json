{
	"name": "Exercise 8 - AutoML with Spark",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool02",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "04c0b970-1330-43d9-963e-9ce4dd781ed8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/c0675ddc-d867-4b54-abe8-24d506dffc0e/resourceGroups/DataMigrationService/providers/Microsoft.Synapse/workspaces/synw-ae-demo/bigDataPools/sparkpool02",
				"name": "sparkpool02",
				"type": "Spark",
				"endpoint": "https://synw-ae-demo.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool02",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Analyze and prepare the customer ratings dataset\n",
					"\n",
					"Use `spark.read.csv()` to load the data from the source public blob storage account and display its schema and shape."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.types import *\n",
					"import numpy as np\n",
					"import pandas as pd\n",
					"\n",
					"manualSchema = StructType([\n",
					"  StructField(\"CustomerId\", StringType(), True),\n",
					"  StructField(\"ProductId\", StringType(), True),\n",
					"  StructField(\"Rating\", LongType(), True),\n",
					"  StructField(\"Cost\", FloatType(), True),\n",
					"  StructField(\"Size\", FloatType(), True),\n",
					"  StructField(\"Price\", FloatType(), True),\n",
					"  StructField(\"PrimaryBrandId\", LongType(), True),\n",
					"  StructField(\"GenderId\", LongType(), True),\n",
					"  StructField(\"MaritalStatus\", LongType(), True),\n",
					"  StructField(\"LowerIncomeBound\", FloatType(), True),\n",
					"  StructField(\"UpperIncomeBound\", FloatType(), True)\n",
					"])\n",
					"\n",
					"url = \"wasbs://files@synapsemlpublic.blob.core.windows.net/PersonalizedData.csv\"\n",
					"raw_data = spark.read.csv(url, header=True, schema=manualSchema)\n",
					"print(\"Schema: \")\n",
					"raw_data.printSchema()\n",
					"\n",
					"df = raw_data.toPandas()\n",
					"print(\"Shape: \", df.shape)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Take a look at some of the items in the dataset. Notice the two-class ratings (0 vs. 1) provided by customers to products.\n",
					"\n",
					"The goal of this exercise is to build a Machine Learning classification model capable of predicting the rating based on Cost, Size, Price, PrimaryBrandId, GenderId, MaritalStatus, LowerIncomeBound, and UpperIncomeBound. To achieve the goal, you will use Azure Machine Learning (AML) automated machine learning (Auto ML)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(df.iloc[:10, :])"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Split the data into the train and test parts using a ratio of 80% train to 20% test.\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"split_ratio = 0.8\n",
					"seed = 42\n",
					"raw_train, raw_test = raw_data.randomSplit([split_ratio, 1 - split_ratio], seed=seed)\n",
					"print(\"Train: (rows, columns) = {}\".format((raw_train.count(), len(raw_train.columns))))\n",
					"print(\"Test: (rows, columns) = {}\".format((raw_test.count(), len(raw_test.columns))))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Use the subscription id, resource group name, AML workspace name, and AML workspace region from your environment to connect to the AML workspace. Make sure the values are identical to the ones displayed in the Azure portal."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from azureml.core import Workspace, Experiment\n",
					"\n",
					"# Enter your workspace subscription, resource group, name, and region.\n",
					"subscription_id = \"c0675ddc-d867-4b54-abe8-24d506dffc0e\"\n",
					"resource_group = \"datamigrationservice\"\n",
					"workspace_name = \"amlworkspace\"\n",
					"workspace_region = \"eastus\"\n",
					"\n",
					"ws = Workspace(workspace_name = workspace_name,\n",
					"               subscription_id = subscription_id,\n",
					"               resource_group = resource_group)\n",
					"\n",
					"experiment_name = \"aml-synapse-classification\"\n",
					"experiment = Experiment(ws, experiment_name)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Persist the train and test datasets as CSV files and upload them to the AML data store.\n",
					"\n",
					"Load the train dataset as an AML tabular dataset (this format is used by the AutoML run)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pandas \n",
					"from azureml.core import Dataset\n",
					"\n",
					"# Get the Azure Machine Learning default datastore\n",
					"datastore = ws.get_default_datastore()\n",
					"\n",
					"train_pd = raw_train.toPandas()\n",
					"train_pd[train_pd.columns[2:]].to_csv('train.csv', index=False)\n",
					"test_pd = raw_test.toPandas()\n",
					"test_pd[test_pd.columns[2:]].to_csv('test.csv', index=False)\n",
					"\n",
					"# Convert into an Azure Machine Learning tabular dataset\n",
					"datastore.upload_files(files = ['train.csv', 'test.csv'],\n",
					"                       target_path = 'train-dataset/tabular/',\n",
					"                       overwrite = True,\n",
					"                       show_progress = True)\n",
					"ds_train = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-dataset/tabular/train.csv')])"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Use AML Auto ML to train the classification model"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Configure the AutoML run to use at most 20 iterations (combinations of ML algorithms and hyper-parameter values). This limitation will ensure the AutoML run will not exceed a total run time of 7-8 minutes.\n",
					"\n",
					"The `enable_onnx_compatible_models` ensures the run produces a model that is ONNX compatible. This will make the model available for inference directly on dedicated SQL pool tables, via the AML linked service configured in Synapse."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"import logging\n",
					"\n",
					"automl_settings = {\n",
					"    \"iterations\": 5,\n",
					"    \"iteration_timeout_minutes\": 5,\n",
					"    \"experiment_timeout_minutes\": 15,\n",
					"    \"max_concurrent_iterations\": 2,\n",
					"    \"enable_early_stopping\": True,\n",
					"    \"enable_onnx_compatible_models\": True,\n",
					"    \"primary_metric\": 'accuracy',\n",
					"    \"featurization\": 'auto',\n",
					"    \"verbosity\": logging.INFO,\n",
					"    \"n_cross_validations\": 2}"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Finalize the configuration of the AutoML run. Specify the task type (`classification`), the data to train on, and the compute resource to use. In this case, `spark_context = sc` specifies that the AutoML run will use the local Spark pool as the compute resource to run the entire process. \n",
					"The AML workspace is still coordinating the whole process, but the compute being used is local."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from azureml.train.automl import AutoMLConfig\n",
					"\n",
					"automl_config = AutoMLConfig(task='classification',\n",
					"                             debug_log='automated_ml_errors.log',\n",
					"                             training_data = ds_train,\n",
					"                             spark_context = sc,\n",
					"                             model_explainability = True, \n",
					"                             label_column_name =\"Rating\",**automl_settings)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Submit the AutoML run and wait for its completion. The settings were chosen in a way that the total run time should not exceed 7-8 minutes. While the experiment is running, go ahead and open the Azure Machine Learning Studio in the Azure portal and check out the details of the AutoML run.\n",
					"\n",
					"Once the run completes, check the list of trained models and their performance metric (`accuracy` in our case)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from azureml.core.experiment import Experiment\n",
					"\n",
					"# Start an experiment in Azure Machine Learning\n",
					"tags = {\"Synapse\": \"classification\"}\n",
					"local_run = experiment.submit(automl_config, tags = tags)\n",
					"local_run.wait_for_completion(show_output=True)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Register the best model in the AML workspace"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Retrieve the best model and its associated child run from the AutoML run. Retrieve all the files associated with the child run and inspect its properties."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Get best model\n",
					"best_run, fitted_model = local_run.get_output()\n",
					"best_run.download_files(output_directory='./best_run')\n",
					"best_run.properties"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Use MLFlow to define the signature of the model and inspect it.\n",
					"\n",
					"The signature will be later used to generate the MLFlow model file. This file is important because it is used by the UI for the integration of the AML experience in Synapse Studio."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import mlflow\n",
					"from mlflow.types import DataType, ColSpec, TensorSpec, Schema\n",
					"from mlflow.models import ModelSignature\n",
					"\n",
					"model_signature = ModelSignature(\n",
					"    Schema([\n",
					"        ColSpec(DataType.float, 'Cost'),\n",
					"        ColSpec(DataType.float, 'Size'),\n",
					"        ColSpec(DataType.float, 'Price'),\n",
					"        ColSpec(DataType.long, 'PrimaryBrandId'),\n",
					"        ColSpec(DataType.long, 'GenderId'),\n",
					"        ColSpec(DataType.long, 'MaritalStatus'),\n",
					"        ColSpec(DataType.float, 'LowerIncomeBound'),\n",
					"        ColSpec(DataType.float, 'UpperIncomeBound')\n",
					"    ]),\n",
					"    Schema([\n",
					"        ColSpec(DataType.long, 'label_out')\n",
					"    ])\n",
					")\n",
					"\n",
					"model_signature"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Use MLFlow to register the model into the AML model registry.\n",
					"\n",
					"Using MLFlow instead of the standard model registration feature of the AML SDK ensures that all requirements are met for an optimal experience in the UI (when using the integration of AML into Synapse Studio).\n",
					"\n",
					"Note how we are registering the ONNX version of the best model (the user experience integration of AML into Synapse Studio only supports ONNX models)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import onnx\n",
					"\n",
					"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
					"mlflow.set_experiment(experiment_name)\n",
					"artifact_path = 'outputs'\n",
					"\n",
					"xmodel = onnx.load('./best_run/outputs/model.onnx')\n",
					"\n",
					"with mlflow.start_run() as run:\n",
					"    # Save the model to the outputs directory for capture\n",
					"    mlflow.onnx.log_model(xmodel, artifact_path, signature=model_signature)\n",
					"    # Register the model to AML model registry\n",
					"    mlflow.register_model(\"runs:/\" + run.info.run_id + \"/\" + artifact_path, \"aml-synapse-classifier\")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Persist test data to the dedicated SQL pool\n",
					"\n",
					"Create a temporary view in PySpark on top of the `raw_test` dataset. This is required to allow the `synapsesql` extension in Scala to read the data that will be persisted into a dedicated Synapse SQL pool."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"raw_test.createOrReplaceTempView(\"TestData\")"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Persist the data to the dedicated SQL pool using `synapsesql`."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"microsoft": {
						"language": "scala"
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%spark\n",
					"\n",
					"import com.microsoft.spark.sqlanalytics.utils.Constants\n",
					"\n",
					"val df = spark.sqlContext.sql(\"select * from TestData\")\n",
					"\n",
					"df.write.\n",
					"  synapsesql(\"synapsesql.wwi_ml.CustomerTest\", Constants.INTERNAL)"
				],
				"execution_count": 15
			}
		]
	}
}